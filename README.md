# Dual-Channel Calibration Method for Lidar-Camera System
  This method primarily presents the design of an entirely new multi-channel, highly robust, and high-precision LiDAR-vision calibration system. It is mainly divided into cross-modal pixel-level registration based on learning methods and cross-modal object-level matching based on monocular depth estimation, ultimately using a multi-objective optimization algorithm to obtain accurate extrinsic parameters. Most research is based on learning to regress extrinsic parameters from LiDAR projection maps and camera image features, which requires an initial guess of extrinsic parameters to project the point cloud onto the camera plane and has poor generalization performance. Object-level methods detect each target in 2D-3D dimensions and then optimize the extrinsic parameter values under cross-modal conditions with consistent detected targets. The accuracy of these methods depends on the targets, and the cross-modal association of target features from different modalities is complex. Based on this current situation, this study proposes a dual-channel fusion method, which, after experimental validation, shows significantly higher calibration accuracy than single-channel methods, and further improves calibration accuracy through cascading measures.
## Overview
![image](https://github.com/INDEX108/Dual-channel_Calib/assets/53263493/2f5cd5ed-eeee-401c-b2fa-5f0e44054936)
# Dual-Channel Calibration Method for Multi-Sensor Systems

## Overview

This repository introduces a cutting-edge dual-channel calibration technique for multi-sensor systems, with a focus on high-precision spatial alignment, particularly beneficial for autonomous navigation and robotics applications.

## Key Features

- **Deep Learning-Based Pixel-to-Point Cloud Registration**: Harnesses the power of deep learning for accurate alignment between pixels and point clouds.
- **Monocular Depth Estimation**: Converts monocular depth images into pseudo point clouds for enhanced matching capabilities.
- **Confidence-Model-Based Resampling**: Applies a novel filtering approach to refine the quality of pseudo point clouds.
- **Multi-Objective Optimization**: Leverages the NSGA-III algorithm to find a balance among multiple calibration objectives.

## Dual-Channel Approach

### Channel 1: Deep Learning-Based Registration
- **Input**: Composite structure of images and point clouds.
- **Method**: Utilizes deep learning to achieve precise pixel-to-point cloud registration.

### Channel 2: Monocular Depth and Target Detection
- **Pseudo Point Cloud Generation**: Transforms monocular depth images into pseudo point clouds based on projection relationships.
- **Confidence-Model-Based Resampling**: Enhances pseudo point clouds using a confidence model.
- **Target Detection**: Employs PV-RCNN for camera pseudo point cloud target detection and integrates LiDAR point cloud target detection results.

## Fusion and Optimization

- **2D to 3D Matching**: Combines 2D target proposal boxes (generated by SAM + Deformable DETR) with 3D target detection frame information.
- **Cross-Modal Registration**: Integrates pixel-level cross-modal registration relationships and point set pair scoring.
- **Multi-Objective Joint Optimization**: Applies a method based on NSGA-III for optimizing multiple objectives.
- **Optimal Solution Selection**: Selects a balanced solution from the Pareto front as the final output, calculating the ultimate extrinsic parameters.

## Getting Started

To begin using the Dual-Channel Calibration Method, follow these steps:

1. **Clone the Repository**:
   ```sh
   git clone https://github.com/index108/Dual-Channel-Calibration-Method.git
   ```

2. **Install Dependencies**: Ensure all required libraries are installed as listed in `requirements.txt`.

3. **Configure Sensor Parameters**: Adjust settings in `config.py` to align with your specific sensor configuration.

4. **Run Calibration**: Execute the calibration process using the provided scripts.

5. **Evaluate and Iterate**: Analyze the results and refine the calibration as necessary.

## Contributing

We welcome contributions to this project. To submit a contribution, please open an issue or submit a pull request detailing your enhancements, bug fixes, or new features.

## License

This project is released under the [MIT License](LICENSE).

## Acknowledgements

Our gratitude extends to [relevant institution or funding body], the open-source community, and the authors of foundational research that has inspired and informed this work.

---

Please replace placeholders with your actual GitHub username and repository URL. Include specific files, scripts, or documentation in your repository with clear instructions for their use.
